# =============================================================================
# LOCAL SETUP WITH OLLAMA
# =============================================================================
# Run models locally using Ollama. No API keys needed!
# First install Ollama: https://ollama.ai
# Then pull models: ollama pull llama3.1, ollama pull mistral, etc.
# =============================================================================

# Ollama host (setting this enables Ollama)
OLLAMA_HOST=http://localhost:11434

# Model priority for each tier
TIER1_MODELS=ollama:llama3.1:8b
TIER2_MODELS=ollama:llama3.1:70b
TIER3_MODELS=ollama:llama3.1:70b

# =============================================================================
# ALTERNATIVE: Different local models per tier
# =============================================================================
# TIER1_MODELS=ollama:llama3.1:8b
# TIER2_MODELS=ollama:mistral
# TIER3_MODELS=ollama:llama3.1:70b

# =============================================================================
# HYBRID: Local + Cloud Fallback
# =============================================================================
# Use local Ollama as primary, fall back to cloud if local fails
# =============================================================================

# OPENAI_API_KEY=sk-your-openai-key-here
# OLLAMA_HOST=http://localhost:11434
#
# TIER1_MODELS=ollama:llama3.1:8b,openai:gpt-4o-mini
# TIER2_MODELS=ollama:llama3.1:70b,openai:gpt-4o
# TIER3_MODELS=ollama:llama3.1:70b,openai:gpt-4o
