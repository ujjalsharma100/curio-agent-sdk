# =============================================================================
# RECOMMENDED TEMPLATE - Multi-Provider Configuration with SQLite
# =============================================================================
# This is a comprehensive template with best practices for production use.
# Copy this file to your project root as `.env` and update with your API keys.
# =============================================================================

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# SQLite is recommended for development and small-to-medium deployments.
# For production at scale, consider PostgreSQL (see commented section below).

# Database type: sqlite, postgres, or memory
DB_TYPE=sqlite

# SQLite database file path (relative or absolute)
DB_PATH=./agent_sdk.db

# Database connection pool settings
DB_MIN_CONNECTIONS=1
DB_MAX_CONNECTIONS=10

# -----------------------------------------------------------------------------
# PostgreSQL Configuration (for production - uncomment and configure)
# -----------------------------------------------------------------------------
# DB_TYPE=postgres
# DB_HOST=localhost
# DB_PORT=5432
# DB_NAME=agent_sdk
# DB_USER=postgres
# DB_PASSWORD=your-postgres-password
# DB_SCHEMA=agent_sdk

# =============================================================================
# LLM PROVIDER API KEYS
# =============================================================================
# Set API keys for all providers you want to use.
# You can use one, two, or all three providers for redundancy.

# OpenAI API Key
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic API Key
# Get your key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# Groq API Key
# Get your key from: https://console.groq.com/
GROQ_API_KEY=gsk_your-groq-api-key-here

# =============================================================================
# PROVIDER CONFIGURATION
# =============================================================================
# Configure default models and enable/disable providers

# OpenAI Configuration
OPENAI_DEFAULT_MODEL=gpt-4o-mini
OPENAI_ENABLED=true
# OPENAI_BASE_URL=https://api.openai.com/v1  # Optional: custom endpoint

# Anthropic Configuration
ANTHROPIC_DEFAULT_MODEL=claude-3-haiku-20240307
ANTHROPIC_ENABLED=true

# Groq Configuration
GROQ_DEFAULT_MODEL=llama-3.1-8b-instant
GROQ_ENABLED=true

# =============================================================================
# TIER-BASED ROUTING CONFIGURATION
# =============================================================================
# The order in TIER*_MODELS IS the priority - first model is tried first.
# If it fails or hits rate limits, the next model is automatically tried.
# You can mix providers in any order!

# -----------------------------------------------------------------------------
# Tier 1: Fast/Cheap Models
# Best for: Simple tasks, synthesis, quick lookups, tool execution
# -----------------------------------------------------------------------------
# Priority: Groq (fastest) -> OpenAI mini (cheap) -> Anthropic Haiku (fast)
TIER1_MODELS=groq:llama-3.1-8b-instant,openai:gpt-4o-mini,anthropic:claude-3-haiku-20240307
TIER1_ENABLED=true

# -----------------------------------------------------------------------------
# Tier 2: Balanced Models
# Best for: General reasoning, moderate complexity tasks, planning
# -----------------------------------------------------------------------------
# Priority: Groq 70B (fast & capable) -> OpenAI GPT-4o (balanced) -> Anthropic Sonnet (high quality)
TIER2_MODELS=groq:llama-3.3-70b-versatile,openai:gpt-4o,anthropic:claude-3-5-sonnet-20241022
TIER2_ENABLED=true

# -----------------------------------------------------------------------------
# Tier 3: High Quality Models
# Best for: Complex reasoning, critical planning, analysis, synthesis
# -----------------------------------------------------------------------------
# Priority: Anthropic Sonnet (best quality) -> OpenAI GPT-4o (excellent) -> Groq 70B (capable)
TIER3_MODELS=anthropic:claude-3-5-sonnet-20241022,openai:gpt-4o,groq:llama-3.3-70b-versatile
TIER3_ENABLED=true

# =============================================================================
# MULTI-KEY ROTATION (Optional)
# =============================================================================
# For high-volume usage, configure multiple API keys per provider.
# The SDK automatically rotates through healthy keys to avoid rate limits.
# Uncomment and configure as needed.

# Multiple Groq keys
# GROQ_API_KEY_1=gsk_first-groq-key-here
# GROQ_API_KEY_1_NAME=groq-primary
# GROQ_API_KEY_1_ENABLED=true
# GROQ_API_KEY_2=gsk_second-groq-key-here
# GROQ_API_KEY_2_NAME=groq-secondary
# GROQ_API_KEY_2_ENABLED=true

# Multiple OpenAI keys
# OPENAI_API_KEY_1=sk-first-openai-key-here
# OPENAI_API_KEY_1_NAME=openai-primary
# OPENAI_API_KEY_1_ENABLED=true
# OPENAI_API_KEY_2=sk-second-openai-key-here
# OPENAI_API_KEY_2_NAME=openai-secondary
# OPENAI_API_KEY_2_ENABLED=true

# Multiple Anthropic keys
# ANTHROPIC_API_KEY_1=sk-ant-first-key-here
# ANTHROPIC_API_KEY_1_NAME=anthropic-primary
# ANTHROPIC_API_KEY_1_ENABLED=true
# ANTHROPIC_API_KEY_2=sk-ant-second-key-here
# ANTHROPIC_API_KEY_2_NAME=anthropic-secondary
# ANTHROPIC_API_KEY_2_ENABLED=true

# =============================================================================
# GENERAL CONFIGURATION
# =============================================================================

# Default tier to use when tier is not specified
DEFAULT_TIER=tier2

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Maximum retries for LLM calls
MAX_RETRIES=10

# Request timeout in seconds
REQUEST_TIMEOUT=60

# =============================================================================
# ON-PREM / CUSTOM LLM CONFIGURATION (Optional)
# =============================================================================
# For on-premise or custom LLM deployments, you can configure custom providers
# programmatically. See examples/custom_onprem_provider.py for details.
#
# If using OpenAI-compatible API, you can use:
# OPENAI_BASE_URL=https://your-onprem-endpoint.com/v1
# OPENAI_API_KEY=your-onprem-key
#
# Then use "openai" provider with your custom base_url.

# =============================================================================
# OLLAMA CONFIGURATION (Optional - for local models)
# =============================================================================
# Uncomment to use local Ollama models instead of or alongside cloud providers

# OLLAMA_HOST=http://localhost:11434
# OLLAMA_DEFAULT_MODEL=llama3.1:8b
# OLLAMA_ENABLED=false

# If using Ollama, you can add it to your tiers:
# TIER1_MODELS=groq:llama-3.1-8b-instant,ollama:llama3.1:8b,openai:gpt-4o-mini

# =============================================================================
# NOTES
# =============================================================================
#
# 1. Model Priority: The order in TIER*_MODELS matters! First model is tried first.
#
# 2. Failover: If a model fails or hits rate limits, the next model in the list
#    is automatically tried. This provides built-in redundancy.
#
# 3. Cost Optimization: Tier 1 uses cheaper/faster models, Tier 3 uses best quality.
#    Configure your agent to use appropriate tiers for different tasks.
#
# 4. Multi-Provider Benefits:
#    - Automatic failover if one provider is down
#    - Rate limit handling across providers
#    - Cost optimization (use cheaper models when appropriate)
#    - Performance optimization (use faster models when appropriate)
#
# 5. Key Rotation: Multiple API keys per provider help avoid rate limits and
#    provide redundancy. The SDK automatically rotates through healthy keys.
#
# 6. Database: SQLite is fine for development. For production at scale,
#    use PostgreSQL for better performance and concurrent access.
#
# =============================================================================
# QUICK START
# =============================================================================
#
# 1. Copy this file to your project root as `.env`
# 2. Replace placeholder API keys with your actual keys
# 3. Adjust tier models based on your needs
# 4. Run your agent!
#
# Example:
#   cp examples/recommended_template.env .env
#   # Edit .env with your API keys
#   python your_agent.py
#
# =============================================================================

